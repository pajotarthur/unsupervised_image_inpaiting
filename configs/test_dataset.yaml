datasets:
  train:
    _name:  &dataset_name celebA
    train: train
    batch_size: &train_batch_size 16
    size: 1000
    im_size: &im_size 64
  #    root: &dataset_root /local/pajot/data/celeba-data
  eval:
    _name: *dataset_name
    train: val
    batch_size: 32
    size: 128
    im_size: *im_size
    shuffle: False
#    root: *dataset_root

experiment:
  _name: gan_expe
  nepoch: 5
  nz: &nz 16
  num_sample: 8
  niter: 50
  gan_mode: vanilla
  split_D: True
  num_dis_step: 1
  root: /net/cluster/pajot/logs/test/
  G_batch_size: *train_batch_size
  writers:
    - _name: sacred

modules:
  gen:
    _name: biggan_gen
    input_nc: 3
    ngf: 32
    nz: *nz
    init_name: orthogonal
    init_gain: 1.41

  dis:
    _name: biggan_dis
    input_nc: 3
    ndf: 32
    init_name: orthogonal
    init_gain: 1.41

optimizers:
  optim_gen:
    _name: adam
    _modules: gen
    lr: 1.e-4
    betas:
      - 0.0
      - 0.999

  optim_dis:
    _name: adam
    _modules: dis
    lr: 4.e-4
    betas:
      - 0.0
      - 0.999

